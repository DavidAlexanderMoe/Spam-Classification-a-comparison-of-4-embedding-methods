{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|      | Precision | Recall | Accuracy |\n",
    "|------|-----------|--------|----------|\n",
    "| TF-IDF | 100%     | 83.1%  | 97.8%    |\n",
    "| word2vec | 60%     | 22.3%  | 87.7%    |\n",
    "| doc2vec | 86.8%     | 44.6%  | 91.7%    |\n",
    "| RNN | 98.7%     | 96.9%  | 93.3%    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models are not deterministic.\n",
    "\n",
    "- TF-IDF performs really really well! When the model says a text message is spam, it actually is spam 100% of the time. When a text message actually is spam, it identifies it as such 83.1% of the time. 16.9% of real spam would make it through. When classifying spam or ham, the model was correct 97.3% of the time.\n",
    "\n",
    "Same interpretation for the others.\n",
    "\n",
    "- word2vec, due to the averaging we did, is the worst model by far.\n",
    "- RNN was outstanding but also really close to the baseline.\n",
    "\n",
    "This is an importante case study, also when considering the precision-recall calibration. In a problem like fraud detection, you need to optimize for recall since you want to really capture fraud without missing any. In a situation of spam filtering you should optimize for precision: in other words, i can handle the model allowing some spam into my inbox but if it classifies a real message as spam i won't be happy!\n",
    "\n",
    "So in this case, TF-IDF could be the way to go, even over the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key takeaways of these techniques\n",
    "\n",
    "### TF-IDF\n",
    "- Simple\n",
    "- no context\n",
    "- very sparse vectors\n",
    "- good baseline\n",
    "\n",
    "### Word2Vec\n",
    "- shallow NN that creates word vectors that can be averaged to create a document level representation\n",
    "- this averaging causes loss of info\n",
    "- more sophisticated\n",
    "- dense vectors\n",
    "- many more\n",
    "\n",
    "### Doc2vec\n",
    "- document level representation vectors\n",
    "- slower but can be powerful\n",
    "- better than word2vec in sentence level representation\n",
    "- smaller dense \n",
    "- considers context as word2vec with the same window (previous and next n-grams) approach\n",
    "\n",
    "### RNN\n",
    "- perfect for sequential data like sentences\n",
    "- dense vectors are created within the model\n",
    "- powerful also with limited data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
